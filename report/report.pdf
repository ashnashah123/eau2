# Introduction
The eau2 system is a distributed key-value store for big data analysis. There will be applications that can interact with the store in order to get and put data from different nodes that may not fit into memory. The system can read in large chunks of data and create different types of data structures depending on the use case. It can also serialize and deserialize data in order to distribute data between nodes.

# Architecture
There are 3 levels to the eau2 system: the key-value store and networking layer, the data abstraction layer, and the application layer.

Level 1: KV Store and Network Layer
The first and lowest layer deals with the distributed KV store. A KV store runs on each node, creating a distributed system. Each KV store on each node has part of the data, and the nodes talk to each other to exchange data when needed. The networking portion of this layer helps the nodes talk to each other and distribute data when needed. There is a master node that helps tell each node which nodes exist via their IP. The children nodes can pass data around to each other easily (since all the data has already been serialized) after registering with the master node.

Level 2: Data Abstraction Layer
This level provides an abstraction of the data that exists in Level 1. In this layer, data structures exist in a user friendly format, representing the serialized data from Level 1. Level 2 interacts with Level 1 through a KDStore which contains a KVStore. It adds data to the KV Store by serializing it. Any data that will be put in the KV store must first be put into a DataFrame.

Level 3: Application Layer
This layer is where we can do ML and AI and GPUs! Any use case / algorithm that requires data can be created as an application in this layer. It will interact with Layer 2 to manipulate the data. Layer 2 interacts with Layer 1 to take care of distributed data.

# Implementation
## Level 1 implementation:

### Milestone 1:
The Key-value store is basically a map of keys to values. Therefore we plan on implementing a KVStore class with a Map field.

Pseudocode for implementing the KVStore:

```

class KVStore {
    Map map;

    waitAndGet(k) => v
    put(k, v) => void
}
class Key {
    String name;
    size_t home;
}
class Value {
    char* data;
}

```
The KVStore class has a waitAndGet method, which takes a key and returns the data value. Since a key has a "home" and knows on which node it lives, we can get the value of the data there by using our network implementation. So the node requesting data would send a message to the "home" node (the one that has the key's value), and that "home" node would send back the value. In the implementation for the network portion of this, the messages sent between nodes will have specific types / be prefaced with specific words that indicate what kind of operation is happening (registering, asking for data, etc...).
The put(k, v) method would call the Map's put and store data in the same way a map stores data.

The network portion of this layer will consist of a client.cpp and a server.cpp. A client / node has a thisNode() method that returns a number, which is the number that represents which node it is in the network. This number corresponds to the "home" size_t field that exists on a Key.
Each client node has its own KVStore. Because it goes through the registration period with the server node, it knows who all the other nodes are in the system.

### Milestone 2:
We properly implemented the classes outlined in milestone 1 (KVStore, Key, Value).
We also added a KDStore which directly interfaces with the dataframe classes. It has a KVStore as a field which strictly handles serialized data. This way the layers of passing data are separated.

### Milestone 3:
We added a Lock as a field to KVStore to allow for threading. We also made our Application class inherit from Thread to implement threading.
We also properly implemented waitAndGet on the KVStore now that we added threading to our program.

### Milestone 4:
We started with the PseudoNetwork code for this, and attempted to get it to work. There is a lot of debugging we are sifting through, but we implemented the logic of distributing data over the pseudonetwork. Each application inherits from the Thread class, and each application is spun up in the main method. The KVStore also now holds the network object so that it can interface with the network. 

### Milestone 5:
We worked partially through the PseudoNetwork and then moved onto integrating the real network with this. We have implemeted a lot of logic in the KVStore which holds a network and uses it to interface with the map that holds all the data.

For the network, we implemented:
- the Network_Ip class which contains methods for server and client init. They create the sockets and bind them to the appropriate ports. 
- more message types. We now have: DATA, DATA_REQUEST, REGISTER, DIRECTORY. The DATA message is when one node sends data to another node and the DATA_REQUEST is for when a node requests data from another node. 

In the KVStore:
- We now have the KVStore inheriting from Thread so that it can run in its own loop as it waits for messages to come in.
    - When a DATA message comes in, the data is added to this kvstore's map. It is stored at its original key, as we do not intend to make a copy of the data, and we intend to put it in this kvstore's map as if this map was a cache. We plan to implement a cache eventually and this will have a time limit on number of items it holds, that way one node does not end up holding data from every single node. If the data belongs on the node (i.e. the incoming key's home is the same as the current node num, then the message will be put into the kvstore.) Both the put_in_cache and put_ methods put into the map, but when we implement a caching mechanism, we plan to make the cache separate. 
    
    - When a DATA_REQUEST message comes in, the kvstore has to look in its map for the data at the requested key. The waitAndGet method is called on the requested key, as it may not be in this node just yet. For example, the clients request the DF immediately from node 0 (server), so if the server just calls get() right away, it might not actually have the DF just yet as it is still reading in the file. Hence, anyone who needs to get data and send it to someone else should call waitAndGet just as a safety check in case the data has not yet been added to the map. 
    
    - the distribute_df_to_all() method is called in the fromVisitor function and is meant for the DF to be distributed to all nodes. 

## Level 2 implementation:

### Milestone 1:
Since data will be put into the KVStore as a serialized DataFrame, we have methods that convert data from one form into a DataFrame. For example, the  methods on datastructures fromArray, fromScalar, etc... convert an Array of data and a Scalar of data into a DataFrame, respectively. We can expand these to also include other datatypes.
The classes that we will include here are all our serialization classes: Serialize.h and Deserialize.h. In addition, we will have the DataFrame and all classes associated with the DataFrame (such as Schema, Lists, Columns, Rows, Rowers, and Fielders, etc...).

### Milestone 2:
We implemented serialization for the dataframe and related classes. 
We also implemented fromArray. We will probably be able to implement new methods like fromScalar, etc. more easily now. 

### Milestone 3:
We implemented fromScalar in order to get the example given from class to run.

### Milestone 4:
We refactored a lot of code for this milestone. We cleaned up our array classes as well as columns. Our columns now hold keys which point to chunks of data, which are our newly refactored Array classes. This helped us with the "distributed" portion of the assignment. We also did this so that the DataFrame API would not have to change too much because the column classes took care of the interfacing with the KVStore. We added all the classes given to us for the WordCounter example.

### Milestone 5:
We did not change very much in this layer, but as we were writing code for our Column classes, we noticed that we should refactor these classes, as we started having a lot of repeated code. Due to time constraints and wanting to move forward with functionality, we did not refactor the columns yet, but plan to in the upcoming week. 

## Level 3 implementation:
Because Level 3 is the layer where the user writes code that sits on top of the first two players, Level 3 is mostly use cases, which can be seen in the section below.

### Milestone 3:
We copied in the code suggested to get to run and ran it with threads to prepare for the network section of Milestone 4.

### Milestone 4:
We now have a main method which is used to spin up multiple WordCounters running on different threads. The WordCount API itself deals with mocking the nodes and kicking off each application. 

### Milestone 5:
We have server and client .cpp files. They each have a main method which reads in arguments from the command line (example of how to run this is below in the STATUS section). After reading in args from the command line, an instance of a NetworkIp is created and server_init/client_init is called with the appropriate parameters from the command line. They each take the index of the node, the port, and their IP address. The client also takes the server's port and ip address. A KVStore is then initialized, with the network and node index as its parameter. The WordCount application is also initialized with the kvstore as its parameter as well as the node index. 

The WordCount then calls start_kv() which spins up the thread for the KVStore and puts the KVStore in an infinite listening loop as it listens for messages. 

The WordCount then calls its own run_() method to kick off the WordCount application. The WordCount class no longer inherits from Thread, as only the KVStore needs a thread. 

# Use cases
The example of the system that computes a sum and checks the computation is one use case.

Other use cases could also have similar math operations. For example, any type of filter or map method can be implemented on the data. The problem statement of 'find me all broken pPhones that were assembled by workers between 7 and 17 years old' is a very specific fitlering use case that can be run on large amounts of data.

## Milestone 2:
A successful use case is summing/subtracting ints. It was the trivial example given to us in the assignment.

## Milestone 3:
A successful use case is what was given to us to run for this assignment. This use case can be seen in the Demo.h file under src/application_layer

## Milestone 4:
The WordCount is a use case for this. 

## Milestone 5:
The WordCount and Linus applications are use cases for this. 

# Open questions

## Milestone 5
- How do we shut down the program nicely? Where do we call the teardown?
- How are we supposed to have the client and server "listen" for messages nicely without just using an infinite while loop? (In KVStore's run_() method)


# Status
We worked on getting the real network layer up and running with the WordCounter app. We have the code/logic written for implementing the WordCounter with a Network, but have been making our way through bugs during serialization and deserialization and sending dataframes between nodes.

We are not currently running the Linus application. Once we finish the network layer and get it working, we will start on Linus for the final code walk.

Since we are in the process of debugging, we have hard-coded one server and one client default values into the server.cpp and client.cpp. An example of how to run one server and 2 clients is below. To run the client app:
```
./client -nodes 3 -index 1 -port 8081 -masterip 127.0.0.1 -masterport 8080 -ip 127.0.0.2
./client -nodes 3 -index 2 -port 8082 -masterip 127.0.0.1 -masterport 8080 -ip 127.0.0.3
```
And to run the server app:
```
./server -nodes 3 -index 0 -port 8080 -ip 127.0.0.1
```
Argument explanation:
-nodes is the number of nodes. We currently have a global variable called NUM_NODES that we set, but we will make this dynamic for the final project.
-index is the index number of the node
-port is the port of the node
-masterip is the ip address of the server
-masterport is the port of the server
-ip is the ip address of the client / server

These are run from different terminals. In our server.cpp and client.cpp, we currently are taking in the command line arguments in that exact order with the flags. We will eventually do command line parsing for the final project.

- ** NOTE: We commented out the consumption of val_ in get() in string.h. Now you can call get() as many times as you want. Didn't want to waste time debugging right now. **

## Work that will remain unfinished:
- Implementing the Linus application
- Abstracting some code
- Fixing valgrind errors

## Milestone 5 Notes/Work Completed:
- Deleted out set in DF and Columns - no longer needed
- Assumption we have made: When we receive data from a different node, we store it in our kvstore at the old key that is given to us. We don't think it really makes sense that the data should be stored at a new key that reflects `this node` since then it gets confusing as to who actually owns the data (the other node or this node?) We also don't want to reset the key to this node because eventually this data will be stored in a cache, and if we flush the cache we will toss all the data as it will no longer live on its original node. Our put_ method in kvstore then takes a parameter that determines if it was initially distributed by node 0. If distributed by node 0 (in the beginning in fromVisitor) then there is a check for the home of the key so that the node knows it should put the data on itself. Otherwise, it was data received by another node as the return for waitAndGet and hence should just be added to this node's kvstore at its original key. 

## Things to run for Jan FCW:
make; make run; // tests
g++ -std=c++11 -Wall -g src/server.cpp -o server
g++ -std=c++11 -Wall -g src/client.cpp -o client
valgrind: 
- turn on docker
docker run -ti -v `pwd`:/test w2-gtest:0.1 bash -c "cd /test ; g++ -std=c++11 src/server.cpp -pthread; valgrind --leak-check=yes ./a.out"